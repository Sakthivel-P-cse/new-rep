import math
import re
from collections import Counter
from typing import Dict, Tuple


LANG_COMMENTS = {
    "Python": {"line": ["#"], "block_start": ["\"\"\"", "'''"], "block_end": ["\"\"\"", "'''"]},
    "C++": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "Java": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "JavaScript": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "C#": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "PHP": {"line": ["//", "#"], "block_start": ["/*"], "block_end": ["*/"]},
    "Ruby": {"line": ["#"], "block_start": [], "block_end": []},
    "Go": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "Swift": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
    "Rust": {"line": ["//"], "block_start": ["/*"], "block_end": ["*/"]},
}

LANG_KEYWORDS = {
    "Python": ["def", "import", "from", "class", "yield", "async", "await", "lambda"],
    "C++": ["#include", "std::", "namespace", "template", "cout", "cin", "::", "auto"],
    "Java": ["public", "static", "void", "class", "new", "import", "package", "System.out"],
    "JavaScript": ["function", "const", "let", "var", "=>", "console.log", "import", "export"],
    "C#": ["using", "namespace", "class", "public", "static", "void", "var", "Console.WriteLine"],
    "PHP": ["<?php", "echo", "function", "use", "namespace", "class"],
    "Ruby": ["def", "end", "class", "module", "require", ":"],
    "Go": ["package", "import", "func", "fmt.", "defer", "go ", "select", "chan"],
    "Swift": ["import", "let", "var", "func", "class", "struct", "print("],
    "Rust": ["fn ", "let ", "mut ", "use ", "pub ", "crate", "println!", "::"],
}


def sigmoid(x: float) -> float:
    return 1.0 / (1.0 + math.exp(-x))


def safe_div(n: float, d: float) -> float:
    return n / d if d else 0.0


def is_english_word(token: str) -> bool:
    return bool(re.fullmatch(r"[A-Za-z]{3,}", token))


def extract_comment_blocks(code: str, language: str) -> Tuple[str, str]:
    spec = LANG_COMMENTS.get(language, {"line": ["#"], "block_start": [], "block_end": []})
    line_markers = spec["line"]
    block_start = spec["block_start"]
    block_end = spec["block_end"]

    comment_lines: list[str] = []
    non_comment_lines: list[str] = []

    in_block = False
    current_block_end = None
    for raw_line in code.splitlines():
        line = raw_line.rstrip("\n")
        stripped = line.strip()
        if in_block:
            comment_lines.append(line)
            if current_block_end and current_block_end in stripped:
                in_block = False
                current_block_end = None
            continue
        # Check block start
        started = False
        for bstart, bend in zip(block_start, block_end):
            if bstart and bstart in stripped:
                comment_lines.append(line)
                in_block = True
                current_block_end = bend
                started = True
                break
        if started:
            continue
        # Check line comment
        if any(stripped.startswith(m) for m in line_markers):
            comment_lines.append(line)
        else:
            non_comment_lines.append(line)

    return ("\n".join(comment_lines), "\n".join(non_comment_lines))


def extract_identifiers(code: str) -> list[str]:
    return re.findall(r"[A-Za-z_][A-Za-z0-9_]*", code)


def ratio_snake_case(identifiers: list[str]) -> float:
    if not identifiers:
        return 0.0
    snake = sum(1 for t in identifiers if "_" in t and t.lower() == t)
    return snake / len(identifiers)


def ratio_camel_case(identifiers: list[str]) -> float:
    if not identifiers:
        return 0.0
    camel = sum(1 for t in identifiers if re.search(r"[a-z][A-Z]", t))
    return camel / len(identifiers)


def ratio_pascal_case(identifiers: list[str]) -> float:
    if not identifiers:
        return 0.0
    pascal = sum(1 for t in identifiers if re.fullmatch(r"[A-Z][A-Za-z0-9]+", t) is not None)
    return pascal / len(identifiers)


def count_string_literals(code: str) -> int:
    patterns = [r'\".*?\"', r"'.*?'", r"`.*?`"]
    count = 0
    for p in patterns:
        count += len(re.findall(p, code, flags=re.DOTALL))
    return count


def detect_ai_markers(text: str) -> float:
    markers = [
        "generated by",
        "created by ai",
        "chatgpt",
        "openai",
        "this function",
        "the following code",
        "as an ai",
    ]
    text_lower = text.lower()
    hits = sum(1 for m in markers if m in text_lower)
    return min(1.0, hits / 3.0)


def keyword_presence_ratio(code: str, language: str) -> float:
    keywords = LANG_KEYWORDS.get(language, [])
    if not keywords:
        return 0.0
    code_lower = code.lower()
    hits = sum(1 for k in keywords if k.lower() in code_lower)
    return hits / len(keywords)


def compute_complexity_ratio(code_without_comments: str) -> float:
    tokens = re.findall(r"\b(if|for|while|switch|case|class|def|function|fn|match|try|catch|except)\b", code_without_comments)
    total_tokens = len(re.findall(r"\b\w+\b", code_without_comments))
    return safe_div(len(tokens), total_tokens)


def count_imports(code: str) -> int:
    patterns = [
        r"^\s*import ", r"^\s*from ", r"^\s*#include ", r"^\s*using ", r"^\s*package ", r"^\s*require ", r"^\s*use "
    ]
    count = 0
    for line in code.splitlines():
        for p in patterns:
            if re.search(p, line):
                count += 1
                break
    return count


def english_comment_density(comment_text: str) -> float:
    words = re.findall(r"[A-Za-z]+", comment_text)
    if not words:
        return 0.0
    english_like = sum(1 for w in words if is_english_word(w))
    return safe_div(english_like, len(words))


def extract_features(code: str, language: str) -> Dict[str, float]:
    code = code.replace("\r\n", "\n").replace("\r", "\n")
    comment_text, code_wo_comments = extract_comment_blocks(code, language)

    lines = code.splitlines()
    total_lines = max(1, len(lines))
    empty_lines = sum(1 for l in lines if not l.strip())
    comment_lines = len(comment_text.splitlines()) if comment_text else 0

    identifiers = extract_identifiers(code_wo_comments)
    avg_identifier_len = safe_div(sum(len(t) for t in identifiers), len(identifiers))

    feature_values: Dict[str, float] = {}
    feature_values["comment_ratio"] = safe_div(comment_lines, total_lines)
    feature_values["avg_line_length_scaled"] = min(1.0, safe_div(sum(len(l) for l in lines), total_lines) / 120.0)
    feature_values["empty_line_ratio"] = safe_div(empty_lines, total_lines)
    feature_values["string_literal_ratio"] = min(1.0, count_string_literals(code) / max(1, total_lines) / 2.0)
    feature_values["ai_marker_ratio"] = detect_ai_markers(code)
    feature_values["docstring_present"] = 1.0 if re.search(r"\"\"\"|'''", comment_text) else 0.0
    feature_values["todo_ratio"] = min(1.0, len(re.findall(r"\b(TODO|FIXME|NOTE)\b", comment_text)) / 10.0)
    feature_values["english_comment_density"] = english_comment_density(comment_text)
    feature_values["snake_case_ratio"] = ratio_snake_case(identifiers)
    feature_values["camel_case_ratio"] = ratio_camel_case(identifiers)
    feature_values["pascal_case_ratio"] = ratio_pascal_case(identifiers)
    feature_values["identifier_avg_length_scaled"] = min(1.0, avg_identifier_len / 20.0)
    feature_values["complexity_ratio"] = min(1.0, compute_complexity_ratio(code_wo_comments))
    feature_values["import_count_scaled"] = min(1.0, count_imports(code) / 20.0)
    feature_values["keyword_presence_ratio"] = keyword_presence_ratio(code, language)

    # whitespace style consistency
    leading = [re.match(r"^[\t ]+", l).group(0) for l in lines if re.match(r"^[\t ]+", l)]
    if leading:
        tab_lines = sum(1 for s in leading if "\t" in s and " " not in s)
        space_lines = sum(1 for s in leading if " " in s and "\t" not in s)
        mixed_lines = len(leading) - tab_lines - space_lines
        consistency = 1.0 - safe_div(mixed_lines, len(leading))
    else:
        consistency = 1.0
    feature_values["indent_consistency"] = consistency

    return feature_values


def analyze_with_weights(code: str, language: str, weights: Dict[str, float]) -> float:
    features = extract_features(code, language)
    bias = weights.get("bias", 0.0)
    score = bias
    for fname, fval in features.items():
        w = weights.get(fname, 0.0)
        score += w * fval
    prob = sigmoid(score)
    return float(max(0.0, min(100.0, prob * 100.0)))

